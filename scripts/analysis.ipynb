{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitd171fe3837a649ac9d856f7ce482dbdd",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "from time import sleep\n",
    "# from features import get_features\n",
    "from mido import MidiFile\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date\n",
    "from time import time, monotonic\n",
    "from os import path, mkdir, listdir, environ\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Args, define evaluation groups\n",
    "\n",
    "iterations = 1        # How many times each experiment should be repeated for the same parameters?\n",
    "max_input_bars = 4    # What is the maximum lookback size for each model?\n",
    "max_output_bars = 4   # What is the maximum output size for each model?\n",
    "# sample_length = 32    # How long of a final composition should be generated?\n",
    "sample_length = 8     # How long of a final composition should be generated?\n",
    "\n",
    "client  = None        # Client Process\n",
    "server  = None        # Server Process\n",
    "expname = ''          # Experiment Name, for naming files\n",
    "\n",
    "ip = \"127.0.0.1\"\n",
    "port = 5005\n",
    "port_in = 57120\n",
    "\n",
    "modelname = None\n",
    "checkpointname = None\n",
    "\n",
    "evaluation_sets = [\n",
    "  # ('bach-chorales', [\n",
    "  ('bach10', [\n",
    "    # ('polyphony_rnn', 'polyphony_rnn'),\n",
    "    # ('pianoroll_rnn_nade', 'rnn-nade_attn'),\n",
    "    ('rl_duet', 'rl_duet'),\n",
    "  ]),\n",
    "  ('piano-e-comp', [\n",
    "    # ('performance_rnn', 'performance_with_dynamics'),\n",
    "    # ('music_transformer', 'performance_with_dynamics')\n",
    "  ]),\n",
    "  (None, [\n",
    "    # ('melody_rnn', 'attention_rnn')\n",
    "  ]),\n",
    "  (None, [\n",
    "    # ('remi', 'remi')\n",
    "  ])\n",
    "]\n",
    "\n",
    "\n",
    "cols_gen = [ \"model\", \"checkpoint\", \"dataset\", \"primer\", \"iteration\", \"out_file\", \"time\", \"in_len\", \"out_len\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Definitions\n",
    "primerdir = ''\n",
    "# outputdir = path.join(path.pardir,'output')\n",
    "# datasetdir = path.join(path.pardir,'dataset') \n",
    "outputdir = path.join(path.curdir,'output')\n",
    "datasetdir = path.join(path.curdir,'dataset')\n",
    "basefoldername = str(date.today())\n",
    "i = 0\n",
    "while True:\n",
    "    foldername = f'{basefoldername}-{i}'\n",
    "    full_foldername = path.join(outputdir, foldername)\n",
    "\n",
    "    if not path.exists(full_foldername):\n",
    "      mkdir(full_foldername)\n",
    "      break\n",
    "\n",
    "    if not any(listdir(full_foldername)): break\n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "# File Management\n",
    "def get_filename(expname,index,primer=None):\n",
    "  return path.normpath(\n",
    "    f\"{foldername}/{expname}-{index}\"\n",
    "    if primer is None else\n",
    "    f\"{foldername}/{expname}-{primer}-{index}\")\n",
    "\n",
    "def get_primer_filename(name):\n",
    "  return path.join(primerdir,name)\n",
    "\n",
    "def get_midi_filename(expname,index,primer=None):\n",
    "  return path.join(outputdir, f'{get_filename(expname,index,primer)}.mid')\n",
    "\n",
    "def get_pickle_filename(expname,index,primer=None):\n",
    "  return path.join(outputdir, f'{get_filename(expname,index,primer)}.pkl')\n",
    "\n",
    "def save_df(df, filename):\n",
    "    # log(f'Saving dataframe to {filename}')\n",
    "    df.to_pickle(filename)\n",
    "\n",
    "def log(message):\n",
    "    print(f'[batch:{expname}] {message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Experiments\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def generate_sample(primer, bars_input, bars_output, index):\n",
    "  client.reset()\n",
    "  client.load_bars(get_primer_filename(primer), bars_input)\n",
    "\n",
    "  i = 0\n",
    "  start_time = monotonic()\n",
    "  while bars_input + i * bars_output < sample_length:\n",
    "    client.generate(bars_output, 'measures')\n",
    "    i += 1\n",
    "  gentime = monotonic() - start_time\n",
    "\n",
    "  filename = get_filename('generation',index,f'{start_time}-{bars_input}-{bars_output}-{primer}')\n",
    "  client.save(filename)\n",
    "  return filename, gentime\n",
    "\n",
    "def run_iteration(model, checkpoint, dataset, primer, i):\n",
    "  return [[model, checkpoint, dataset, primer, i, *generate_sample(primer, bars_input, bars_output, i), bars_input, bars_output]\n",
    "    for bars_input\n",
    "    in range(1,max_input_bars + 1)\n",
    "    for bars_output\n",
    "    in range(1,max_output_bars + 1)\n",
    "  ]\n",
    "\n",
    "def run_generation(model,checkpoint,dataset):\n",
    "  p = [run_iteration(model,checkpoint,dataset,primer, i) for primer in primers for i in range(iterations)]\n",
    "  out = []\n",
    "  for pp in p:\n",
    "    out.extend(pp)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Client, define model management functions\n",
    "from pythonosc import udp_client, osc_server\n",
    "from pythonosc.dispatcher import Dispatcher\n",
    "\n",
    "class BatchClient(udp_client.SimpleUDPClient):\n",
    "  def __init__(self, logger, ip, port_out, port_in):\n",
    "    udp_client.SimpleUDPClient.__init__(self, ip, port_out, port_in)\n",
    "    dispatcher = Dispatcher()\n",
    "    dispatcher.map('/ok', lambda _: self.server.shutdown())\n",
    "    self.server = osc_server.ThreadingOSCUDPServer((ip, port_in), dispatcher)\n",
    "    self.log = logger\n",
    "\n",
    "  def request(self, endpoint, args):\n",
    "    self.log(f'{endpoint} {\" \".join([str(a) for a in args])}')\n",
    "    self.send_message(endpoint, args)\n",
    "\n",
    "  def start(self):\n",
    "    self.request('/start', [])\n",
    "\n",
    "  def stop(self):\n",
    "    self.request('/stop', [])\n",
    "\n",
    "  def set(self,*args):\n",
    "    self.request('/set', args)\n",
    "\n",
    "  def debug(self,key):\n",
    "    self.request('/debug', [key])\n",
    "\n",
    "  def save(self,filename):\n",
    "    self.request('/save', [filename])\n",
    "    self.wait()\n",
    "\n",
    "  def play(self,note):\n",
    "    self.request('/play', [note + 40])\n",
    "\n",
    "  def run(self,filename):\n",
    "    self.set('output_filename', filename)\n",
    "    self.start() # TODO: on host: unset 'batch_complete'\n",
    "    self.wait()\n",
    "\n",
    "  def reset(self):\n",
    "    self.request('/reset', [])\n",
    "  \n",
    "  def pause(self):\n",
    "    self.request('/pause', [])\n",
    "\n",
    "  def end(self):\n",
    "    self.request('/end', [])\n",
    "\n",
    "  def generate(self, length, unit):\n",
    "    self.request('/generate', [length, unit])\n",
    "    self.wait()\n",
    "\n",
    "  def wait(self):\n",
    "    self.log(\"waiting...\")\n",
    "    self.server.serve_forever()\n",
    "    self.log(\"ok!\")\n",
    "  \n",
    "  def load_bars(self,filename,barcount):\n",
    "    self.request('/load_bars', [filename, barcount])\n",
    "    self.wait()\n",
    "\n",
    "client = BatchClient(log, ip,  port,  port_in)\n",
    "logfile = None\n",
    "def start_model(the_modelname, the_checkpointname):\n",
    "    logfile = open(path.join(outputdir, f'{the_modelname}.log'), 'w')\n",
    "    modelname = the_modelname\n",
    "    checkpointname = the_checkpointname\n",
    "    log(f'Starting model: {modelname} ({checkpointname})')\n",
    "    env = environ.copy()\n",
    "    env['NOT_INTERACTIVE'] = '1'\n",
    "    # server = subprocess.Popen([ './start.sh', model, checkpoint, 'batch' ], cwd=path.pardir, env=env)\n",
    "    server = subprocess.Popen([ './start.sh', modelname, checkpointname, 'batch' ], env=env, stdout=logfile, stderr=logfile)\n",
    "    # server.communicate()\n",
    "    client.wait()\n",
    "    client.set('debug_output', False)\n",
    "    client.set('batch_mode', True)\n",
    "    client.set('trigger_generate', 1)\n",
    "    client.set('batch_unit', 'measures')\n",
    "    client.set('debug_output', False)\n",
    "\n",
    "def stop_model():\n",
    "  if client: client.end()\n",
    "  if server: server.wait()\n",
    "  if logfile: logfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[batch:] Starting experiment suite: ./output/2021-05-26-0\n[]\n        model  checkpoint  dataset  primer  iteration  out_file  time  in_len  \\\ncount       0           0        0       0          0         0     0       0   \nunique      0           0        0       0          0         0     0       0   \n\n        out_len  \ncount         0  \nunique        0  \n[batch:] /end \nDone!\n"
     ]
    }
   ],
   "source": [
    "# Generation Phase\n",
    "\n",
    "import pickle\n",
    "\n",
    "max_primers = None\n",
    "max_primers = 5\n",
    "\n",
    "try:\n",
    "    log(f'Starting experiment suite: {outputdir}/{foldername}')\n",
    "    output = []\n",
    "    for dataset, models in evaluation_sets:\n",
    "        if dataset is None: continue\n",
    "        datapath = path.join(datasetdir, dataset)\n",
    "        primerdir = datapath\n",
    "        if not path.exists(datapath):\n",
    "            print(f'Directory not found: {datapath}, skipping')\n",
    "            continue\n",
    "\n",
    "        primers = listdir(datapath)\n",
    "        if max_primers: primers = primers[:max_primers]\n",
    "\n",
    "        for (model, checkpoint) in models:\n",
    "            expname = model\n",
    "            start_model(model, checkpoint)\n",
    "\n",
    "            e = run_generation(model,checkpoint,dataset)\n",
    "\n",
    "            output += e\n",
    "            stop_model()\n",
    "    print(output)\n",
    "    # Save temp file\n",
    "    with open('output.tmp', 'wb') as f:\n",
    "      pickle.dump(output, f)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(output, columns=cols_gen)\n",
    "    print(df.describe())\n",
    "    save_df(df,path.join(outputdir, 'df_gen'))\n",
    "    \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "  print(\"Terminating...\")\n",
    "  client.pause()\n",
    "finally:\n",
    "  stop_model()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Empty DataFrame\nColumns: [model, in_len, out_len, iteration]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# Analysis Stage\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "outputdir = os.path.abspath(os.path.join(os.curdir, 'output'))\n",
    "df_gen = pd.read_pickle(os.path.join(outputdir, 'df_gen'))\n",
    "metricsfile = os.path.join(outputdir, 'metrics')\n",
    "\n",
    "output_samples_dir  = os.path.join(outputdir, 'samples')\n",
    "output_dataset_dir  = os.path.join(outputdir, 'dataset')\n",
    "\n",
    "# Define External Commands\n",
    "extraction_scriptdir = os.path.abspath(os.path.join(os.path.pardir, 'mgeval'))\n",
    "extraction_script = os.path.join(extraction_scriptdir, 'start.sh') \n",
    "cmd_extraction = [ 'bash', extraction_script, output_samples_dir, output_dataset_dir, metricsfile, str(sample_length) ]\n",
    "\n",
    "preprocess_scriptdir = os.path.abspath(os.path.join(os.path.pardir, 'miditools'))\n",
    "preprocess_script = os.path.join(preprocess_scriptdir, 'midisox_py')\n",
    "cmd_preprocess = lambda _in, _out: [ 'python', preprocess_script, '-m', os.path.abspath(_in), os.path.abspath(_out) ]\n",
    "\n",
    "# Prepare output log file\n",
    "logfile = open(path.join(outputdir, f'analysis.log'), 'w')\n",
    "\n",
    "# Prepare Metrics DF\n",
    "df_metrics = df_gen\n",
    "\n",
    "# Prepare Output Data Fields\n",
    "# for metric in metrics:\n",
    "    # df_metrics[metric] = None\n",
    "metriccolumns = ['model', 'in_len', 'out_len', 'iteration']\n",
    "metrics_initialized = False\n",
    "out = []\n",
    "\n",
    "\n",
    "datadirs = [ output_samples_dir, output_dataset_dir ]\n",
    "\n",
    "# Iterate combinations of input/output windows\n",
    "grouping = df_metrics.groupby(['model', 'in_len', 'out_len', 'dataset', 'iteration'])\n",
    "for (model, inn, outn, dataset, iteration), outfiles in grouping.out_file:\n",
    "    expname = model\n",
    "\n",
    "    for _dir in datadirs:\n",
    "        # Check that directory exists\n",
    "        if not os.path.exists(_dir):\n",
    "            os.mkdir(_dir)\n",
    "\n",
    "        # Clean output directory\n",
    "        for file in glob.glob(os.path.join(_dir,'*')):\n",
    "            # log(f'removing: {file}')\n",
    "            if not os.path.exists(file): continue\n",
    "            if os.path.islink(file): os.unlink(file)\n",
    "            else: os.remove(file)\n",
    "\n",
    "    # Remove previous metrics file if any (for sanity)\n",
    "    if os.path.exists(metricsfile):\n",
    "        os.remove(metricsfile)\n",
    "\n",
    "    # Prepare Baseline Dataset Samples\n",
    "    dataset_samples = glob.glob(os.path.join(os.path.curdir, 'dataset', dataset, '*'))\n",
    "    \n",
    "    # FIXME\n",
    "    dataset_samples = dataset_samples[:len(outfiles.unique())]\n",
    "    for index, filename in enumerate(dataset_samples):\n",
    "        out_filename = os.path.join(outputdir, 'dataset', f'sample-{index}.mid')\n",
    "        subprocess.call(cmd_preprocess(filename, out_filename), stdout=logfile, stderr=logfile, cwd=preprocess_scriptdir)\n",
    "    \n",
    "    # Prepare Output Samples \n",
    "    for o, outfile in enumerate(outfiles.unique()):\n",
    "\n",
    "        # Create link to file\n",
    "        in_filename = os.path.abspath(os.path.join(\"output\", f\"{outfile}.mid\"))\n",
    "        out_filename = os.path.abspath(os.path.join(output_samples_dir, f'sample-{o}.mid'))\n",
    "        # log(f'creating: {file}')\n",
    "        # log('processing: ' + ' '.join(cmd_preprocess(in_filename, out_filename)))\n",
    "        # log(f'({inn} | {outn} bars) processing file: {in_filename} -> {out_filename}')\n",
    "        subprocess.call(cmd_preprocess(in_filename, out_filename), stdout=logfile, stderr=logfile, cwd=preprocess_scriptdir)\n",
    "\n",
    "    # Extract Features\n",
    "    subprocess.call(cmd_extraction, stdout=logfile, stderr=logfile, cwd=extraction_scriptdir)\n",
    "\n",
    "    # Read Extracted features\n",
    "    with open(metricsfile, 'r') as metricsfile_:\n",
    "        row_metrics = json.load(metricsfile_)\n",
    "\n",
    "    # Initialize Metrics (if necessary)\n",
    "    if not metrics_initialized:\n",
    "        metrics_initialized = True\n",
    "        for metric in row_metrics.keys():\n",
    "            metriccolumns.extend([metric + '_kl_div', metric + '_overlap'])\n",
    "    \n",
    "\n",
    "    row = [model, inn, outn, iteration]\n",
    "    # print(row_metrics)\n",
    "    for metric in row_metrics.keys():\n",
    "        [_mean, _std, _kl_div, _overlap, _training_set_kl_div, _training_set_overlap] = row_metrics[metric]\n",
    "        row.extend([ _kl_div, _overlap ])\n",
    "    out.append(row.copy())\n",
    "\n",
    "logfile.close()\n",
    "\n",
    "# Save values to output DF\n",
    "print(pd.DataFrame(out, columns=metriccolumns))\n",
    "df = df_metrics.merge(pd.DataFrame(out, columns=metriccolumns), how='inner', on=['model', 'in_len', 'out_len', 'iteration'])\n",
    "\n",
    "df_metrics.to_pickle(path.join(outputdir, 'df_metrics'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6f5baa1b2719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Calculate Mean Values for each in/out len combination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'primer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iteration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in_len'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_groupby_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'numeric_only'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m     68\u001b[0m                             min_count=-1):\n\u001b[1;32m     69\u001b[0m         new_items, new_blocks = self._cython_agg_blocks(\n\u001b[0;32m---> 70\u001b[0;31m             how, alt=alt, numeric_only=numeric_only, min_count=min_count)\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No numeric types to aggregate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# reset the locs in the blocks to correspond to our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataError\u001b[0m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "# Plotting phase\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "suffix = '_avg'\n",
    "make_heat_maps = False\n",
    "make_kldiv_plots = False\n",
    "make_overlap_plots = False\n",
    "make_time_plots = True\n",
    "\n",
    "# Read DF\n",
    "df = pd.read_pickle(path.join(outputdir, 'df_metrics'))\n",
    "\n",
    "# Calculate Mean Values for each in/out len combination\n",
    "df = df.drop(['out_file', 'primer', 'iteration'], axis=1)\n",
    "df = df.groupby(['model', 'checkpoint', 'dataset', 'in_len', 'out_len']).mean()\n",
    "df = df.add_suffix(suffix).reset_index()\n",
    "\n",
    "# Get in/out len dimensions\n",
    "max_in = max(df['in_len'])\n",
    "max_out = max(df['out_len'])\n",
    "\n",
    "# Get Evaluation Metric Names\n",
    "eval_metrics = ['time' + suffix] + [c for c in list(df.columns) if c.endswith('overlap' + suffix) or c.endswith('kl_div' + suffix)]\n",
    "\n",
    "overlaps = [m for m in eval_metrics if m.endswith('overlap' + suffix)]\n",
    "kl_divs  = [m for m in eval_metrics if m.endswith('kl_div' + suffix)]\n",
    "\n",
    "# Iterate Models\n",
    "grouping = df.groupby(['model', 'checkpoint', 'dataset'])\n",
    "for (model, checkpoint, dataset), inner_df in grouping:\n",
    "    # inner_df = inner_df.groupby(['in_len', 'out_len'])\n",
    "    if not make_heat_maps: continue\n",
    "    for metric in eval_metrics:\n",
    "        mdf = inner_df[['in_len', 'out_len', metric]]\n",
    "        mdf = pd.pivot_table(mdf, values=metric, index=['in_len'], columns=['out_len'], fill_value=0)\n",
    "        plt.figure()\n",
    "        plt.title(f'{metric} - {model} {checkpoint} {dataset}')\n",
    "        # print(sns.heatmap(mdf, cmap='RdYlGn_r', linewidths=0.5, annot=True))\n",
    "        sns.heatmap(mdf, cmap='RdYlGn_r', linewidths=0.5, annot=True)\n",
    "        #plt.pcolor(mdf)\n",
    "        #plt.title(metric)\n",
    "        \n",
    "        # plt.savefig()\n",
    "\n",
    "\n",
    "\n",
    "# make_line_plots\n",
    "\n",
    "plot_sets =[\n",
    "    ('Average Overlaps with Base Dataset', overlaps),\n",
    "    ('Average KL-Divergence against Base Dataset', kl_divs),\n",
    "    # ('Average Generation Time', ['time_avg'])\n",
    "]\n",
    "\n",
    "# TODO: Line Plots\n",
    "# How does Input Length affect each metric for a model\n",
    "for (model, checkpoint, dataset), inner_df in grouping:\n",
    "    in_len_plot = inner_df.loc[inner_df['out_len'] == 1]\n",
    "    x = 'in_len'\n",
    "    if not make_kldiv_plots: continue\n",
    "    for title, cols in plot_sets:\n",
    "        plt.figure()\n",
    "        in_len_plot[[x] + cols].plot(x=x, colormap='jet', markersize=10)\n",
    "        plt.title(model)\n",
    "        plt.legend(title=title, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "for (model, checkpoint, dataset), inner_df in grouping:\n",
    "    out_len_plot = inner_df.loc[inner_df['in_len'] == 1]\n",
    "    x = 'out_len'\n",
    "    if not make_overlap_plots: continue    \n",
    "    for title, cols in plot_sets:\n",
    "        plt.figure()\n",
    "        out_len_plot[[x] + cols].plot(x=x, colormap='jet', markersize=10)\n",
    "        plt.title(model)\n",
    "        plt.legend(title=title, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "# model, checkpoint\n",
    "\n",
    "plots = ['in_len', 'out_len']\n",
    "for l in plots:\n",
    "    cols = ['time_avg']\n",
    "    if not make_time_plots: continue\n",
    "    counterpart = [i for i in plots if i != l][0]\n",
    "    for i in df[counterpart].unique():\n",
    "        time_plot = df.loc[df[counterpart] == i]\n",
    "        time_plot = time_plot[['model', 'checkpoint', l] + cols]\n",
    "        time_plot = pd.pivot_table(time_plot, values=cols, index=[l], columns=['model', 'checkpoint'], fill_value=0)\n",
    "        print(time_plot)\n",
    "        plt.figure()\n",
    "        time_plot.plot()\n",
    "        plt.legend(title=f'{title} for {counterpart} = {i}', bbox_to_anchor=(1.05, 1), loc='upper left')\n"
   ]
  }
 ]
}